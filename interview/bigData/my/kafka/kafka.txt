1.怎么保证kafka传过来的数据之正确的处理一次?
-----结合Storm事务来思考

2.flume 和 kafka什么区别？

3.kafka为什么要分多个partition？

4.kafka和spark Streaming 的整合？
-------重要-----不是很清楚，看kafka和SparkStreaming整合

5.怎么保证数据kafka里的数据安全？(怎么解决kafka的数据丢失的问题)
（丢失）----磁盘存储，数据使用完后的删除的策略
kafka丢失数据的情况
  1). producer 向kafka中生产数据丢失数据  -- 开始事务
  2). sparkStreaming 读取kafka 数据丢失数据

6.kafka的key为null可以吗？

7.怎么往kafka集群写数据的？
------Kafka Sink(使用的是Flume)或者KafKa Spout(如果使用的是Storm)

8.kafka用到的什么设计模式？
----发布订阅模式

9.kafka的原理?如果生产数据是消费数据100倍，该如何处理?

10.flume与kafka区别?

11.有很多消息队列技术，为什么选择kafka ?
----kafka的特性方面回答

12.kafka为什么可以支持那么大的吞吐量，怎么实现的，我直接说不知道。?
----顺序读写，partition的分布式存储

kafka怎么消费前一天的数据
  使用KafkaConsumer.offsetsForTimes要确认集群已开启log.message.timestamp.type参数，并且clien要使用0.10.+的客户端发送数据，
  数据格式和0.9不同了。在消息中增加了一个时间戳字段和时间戳类型。目前支持的时间戳类型有两种：CreateTime 和 LogAppendTime
  前者表示producer创建这条消息的时间；后者表示broker接收到这条消息的时间(严格来说，是leader broker将这条消息写入到log的
  时间)

从kafka出来的数据，怎么放入 hive （静态分区和动态分区   没答好）

kafka自定义分区器
  1. 默认的分区策略
   (1) 如果键值为 null，并且使用了默认的分区器，那么记录将被随机地发送到主题内各个可用的分区上。分区器使用轮询（Round Robin）算法将消息均衡地分布到各个分区上。
   (2) 如果键不为空，并且使用了默认的分区器，那么 Kafka 会对键取 hash 值然后根据散列值把消息映射到特定的分区上。这里的关键之处在于，同一个键总是被映射到同一个分区上，所以在进行映射时，我们会使用主题所有的分区，而不仅仅是可用的分区。这也意味着，如果写入数据的分区是不可用的，那么就会发生错误。但这种情况很少发生。
  2. 自定义分区器
   为了满足业务需求，你可能需要自定义分区器，例如，通话记录中，给客服打电话的记录要存到一个分区中，其余的记录均分的分布到剩余的分区中。我们就这个案例来进行演示。
   (1) 自定义分区器
  (2) 使用自定义分区器

Kafka为什么数据传输快？

zero copy原理及如何使用.
 1). Zero copy在内核层直接将文件内容传送给网络Socket，避免应用层数据拷贝，减小IO开销。
 2). java.nio.channel.FileChannel的transferTo()方法实现zero copy

13. 说说kafka的ISR机制
kafka 为了保证数据的一致性使用了isr 机制

1). leader会维护一个与其基本保持同步的Replica列表，该列表称为ISR(in-sync Replica)，每个Partition都会有一个ISR，
而且是由leader动态维护
2). 如果一个flower比一个leader落后太多，或者超过一定时间未发起数据复制请求，则leader将其重ISR中移除
3). 当ISR中所有Replica都向Leader发送ACK时，leader才commit

14. kafka 是如何清理过期数据的？

kafka的日志实际上是以日志的方式默认保存在/kafka-logs文件夹中的，默认7天清理机制，

日志的真正清理时间。当删除的条件满足以后，日志将被“删除”，但是这里的删除其实只是将

该日志进行了“delete”标注，文件只是无法被索引到了而已。但是文件本身，仍然是存在的，只有当过了`log.segment.delete.delay.ms` 这个时间以后，文件才会被真正的从文件系统中删除

15. kafka如何保证数据的不重复和不丢失？

16. kafka里面存的数据格式都是什么样的？

17. kafka中存的一个是数据文件，一个是索引文件，说说这个？

18. 一条message中包含哪些信息？

包含 header,body。
一个Kafka的Message由一个固定长度的header和一个变长的消息体body组成。
header部分由一个字节的magic(文件格式)和四个字节的CRC32(用于判断body消息体是否正常)构成。
当magic的值为1的时候，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，比如是否压缩、
    压缩格式等等)；
如果magic的值为0，那么不存在attributes属性body是由N个字节构成的一个消息体，包含了具体的key/value消息