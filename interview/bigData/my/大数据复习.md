# 大数据复习

## Scala : 

### 1.Scala六大特性【*】
​		与Java无缝整合
​		类型自动推断
​		支持并发和分布式
​		Trait
​		Match 模式匹配
​		高阶函数
​
### 2.Java 和Scala区别【*】
​	Java面向对象,Scala面向函数和对象
    数据类型 java是强数据类型 Scala是弱数据类型，自动推断

### 3.类和对象
    伴生类、伴生对象
	
	var 可变/val 不可变
     
     静态与非静态
        
### 4.方法和函数
	1).方法定义
	
	2).递归方法 : 显式写出返回类型
	
	3).可变长参数的方法
	
	4).参数有默认值的方法
	
	5).匿名函数 ： val xxx:(String,Int)=>String = ()=>{...}
	
	6).嵌套方法
	
	7).偏应用表达式
	
	8).高阶函数 :
	
		方法的参数是函数  ： def xx(fun) ={}
	
		方法的返回是函数 : 
	
		方法的参数和返回都是函数
	
	9).柯里化函数 val x()() = {....}

### 5.Scala String = java String 

### 6.Scala集合

#### Array 

	1).创建：val arr = new Array...  ; val arry = Array[String]()
	
	2).遍历：for ,foreach 
	
	3).可变： val array = new ArrayBuffer[xx]()

#### List

	1).创建：val list = List[Int](xxx)
	
	2).遍历：for ,foreach 
	
	3).可变 ：val list = new ListBuffer[xxx]


#### Set

	1).创建：val set = Set[xx]()
	
	2).遍历：for ,foreach 
	
	3).可变：val set = mutable.Set[xx]..


​		
#### Map
	1).创建 ：val map = Map[String,Int]((xx,xx),"xx"->"xx")
	
	2).遍历：keys ,values ,获取值 map.get(key) Option[String]--{Some ,None}
	
	3).可变：mutable.Map[xx,xx]

#### Tuple

	1).类似于List，只是每个位置都是一个类型
	
	2).定义：val tuple = new Tuple2(xx,xxx) ；val tuple = Tuple2(xx,xx) ;val tuple = (xx,xx)
	
	3).tuple在Scala中最多支持22位
	
	4).tuple遍历 ： val iter  = tuple.producterIterator
	
	5).tuple2.swap()

### 7.Trait : 【*】

	类似java中接口和抽象类的集合，trait中既可以定义常量和变量，也可以定义方法体的实现和不实现，
	
	class继承trait时，第一个关键字使用extends,之后使用with
	
	Trait不可以传参，Object不可以传参

​			
### 8.模式匹配 【*】
​match ... case..

1).模式匹配中既可以匹配值，也可以匹配类型

2).从上往下匹配，匹配上之后不再匹配
​
3).什么都匹配不上，自动匹配
```
case _=>{xxxx}
```
​
4).匹配过程中有数据自动转换
​		
### 9.偏函数
```
val fun:PartialFun[Int,String] = {
​	case 1 =>{"xxx"}
​	.. 
​	case _ =>{"no...match"}
}
```
​
### 10.样例类

* 使用关键字case class就是样例类

* 自动实现了getter ，setter方法，参数就是属性

* 样例类可以new 也可以不用new

* 自动实现了toString,equels，copy方法

​		
### 11.隐式转换【*】
​	隐式值：implicit修饰的值就是隐式值
​	
​	隐式参数：
​		方法中有些参数是implicit修饰
​		多个参数部分参数是隐式的，必须使用柯里化的方式定义
​	隐式函数：
​		使用关键字implict修饰函数，这个函数就是隐式函数
​				
​	隐式类:implicit作用到类上就是隐式类，隐式类必须定义在object和class内部
​	
### 12.Actor通信模型
* 给Actor发送消息
* Actor之间发送消息

​	

## Spark :

### 1.Spark wordCount  【************************】
```scala
val conf = new SparkConf()
​conf.setMaster(..)
conf.setAppName(..)
​val sc = new SparkContext(conf)
sc.textFile(..).flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).foreach...
```
	

### 2.什么是Spark？
	Spark分布式计算框架，数据可以基于内存计算，底层Spark 核心是RDD 分布式数据集，计算数据模型pipeline管道模型，迭代器模式处理。

### 3.Spark & MR 区别？【************************】
* 1).Spark处理数据可以基于内存计算，MR基于磁盘

* 2).Spark有DAG有向无环图优化

* 3).Spark是粗粒度资源申请，MR是细粒度资源申请

* 4).Spark 算子丰富，各种场景应用。MR中只有map和reduce两端

* 5).Spark Shuffle相对于MRshuffle有灵活实现

### 4.Spark核心RDD【************************】
* RDD:弹性分布式数据集

* 五大特性：

	1).RDD由一系列partitoin组成

	2).RDD之间有依赖关系

	3).算子（函数）作用在partition上

	4).分区器作用在K,V格式的RDD上

	5).partition对外提供最佳的计算位置，利于数据处理的本地化

			
* 注意：
	1).textFile 读取文件的方法底层是调用的MR读取HDFS文件的方法，首先Split，每个split对应一个block,这里一个Split对应RDD中的一个partition

	2).什么是K,V格式的RDD? RDD中的数据是一个个的tuple2元素

	3).哪里体现RDD的弹性？

		①.RDD之间有依赖关系

		②.RDD分区可多可少
		
	4).哪里体现RDD的分布式？

		partition是分布在多个节点上

### 5.Spark中的算子【************************】

#### Transformation:转换算子，懒执行，需要Action算子触发执行
	map,flatMap,filter,sample,distinct,reduceByKey,groupByKey,sortBy,SortByKey,mapPartition,MapPartitonWithIdex,zip,
	zipWithIndex,join,leftOuterJoin,rightOuterJoin,union,subtract,interestion,repartiton,coalesce,cogroup,partitionBy,mapValues
	mapToPair,flatmapToPair,mapPartitionToPair,repartitonAndSortWithPartitions,aggregateByKey,combineByKey
			
#### Action:行动算子，触发Transformation类算子执行，每个application中有一个action算子就有一个job
			foreach,collect,count,take,takeSample,reduce,countByKey,countByValue,first,foreachPartitons,saveAsTextFile
			
#### 持久化算子：

##### cache():
	默认将数据存储在内存中，cache() = persist() = persist(StorageLevel.MEMORY_ONLY)

##### persist()
	
	可以手动指定持久化级别，常用级别：MEMORY_ONLY,MEMORY_ONLY_SER,MEMORY_AND_DISK,MEMORY_AND_DISK_SER
	
	尽量少用 “_2”和“DISK_ONLY”级别
				
##### checkpoint()
	
	当RDD的lineage非常长，计算复杂时，可以对RDD使用checkpoint持久化

	checkpoint默认将数据持久化到磁盘

	checkpoint&persist(StorageLevel.DISK_ONLY):

		1).checkpoint使用外部存储系统管理，persist由spark内部管理

		2).appliation执行完成之后，checkpoint数据不会被清空

		3).persist和cache的数据当application执行完成之后会被清空

		4).checkpoint可以用来保存数据状态
​			
##### cache和persist注意：

* 1).cache()和persist() 持久化单位是partition,都是懒执行，需要action触发执行。
​	
* 2).对RDD进行持久化时，持久化之后，可以赋值给一个变量，下次使用这个变量就是使用持久化的数据

* 3).如果采用第②种方式，持久化算子之后不能紧跟action算子

​			
##### checkpoint执行流程：
* 1).application执行完成之后，Spark框架从后往前回溯

* 2).找到checkpointRDD做标记，回溯完成之后，重新启动job计算checkpoint的数据

* 3).将计算的结果持久化的磁盘中
​	
	优化：对哪个RDD进行持久化最好先cache下
​				

### 6.Spark集群搭建

#### 搭建Standalone集群

#### Spark客户端搭建

#### 基于Yarn运行Spark:
​
 在Spark的客户端$SPARK_HOME/conf/spark-evn.sh中配置：
​	
 export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
​		
### 7.Spark任务提交【************************】

#### Spark基于Standalone提交任务：

##### client:
​				命令：./spark-submit --master spark://mynode1:7077 --class ...jar ..
​						./spark-submit --master spark://mynode1:7077 --deploy-mode client --class jar ...
​				流程：
​					1).在客户端提交Spark任务，Driver在客户端启动
​					2).客户端向Master申请资源，Master找到满足资源的Worker节点，启动Executor
​					3).executor启动之后会反向注册给Driver,Driver掌握计算资源
​					4).Driver发送task，监控task执行回收结果
​				注意：
​					client模式适用于测试，不适用于生产环境，当在客户端提交多个Application时，会有单节点网卡流量激增问题。
​					在客户端可以看到task的执行和结果。
​					
##### cluster:
​				命令：./spark-submit --master spark://mynode1:7077 --deploy-mode cluster --class ..jar ..
​				流程：
​					1).在客户端提交Spark任务，客户端向Master申请启动Driver
​					2).Master随机找到一台Worker节点启动Driver
​					3).Driver向Master申请资源，Master找到满足资源的Worker节点启动Executor
​					4).Executor启动之后，向Driver反向注册，Driver掌握了计算资源
​					5).Driver发送task,监控task执行回收结果
​					
​				注意:
​					cluster模式适用于生产环境，当在客户端提交多个Application时，将单节点网卡流量激增问题分散到集群中。在客户端看不到task的执行
​					和结果，要去webui中查看
​	
#### Spark基于Yarn提交任务：
​
##### client:
​	命令：
```shell script
./spark-submit --master yarn --class ..jar ...
./spark-submit --master yarn-client --class ..jar ...
./spark-submit --master yarn --deploy-mode client --class ..jar ..		
```
	流程：
​		
		1).在客户端提交Sparkapplication，首先Driver是在客户端启动
​		
		2).客户端向ResourceManager申请启动ApplicationMaster,RM收到请求之后，随机找到一台NM节点启动AM
​		
		3).AM启动之后，会向RM申请资源用于启动Executor
​		
		4).RM收到请求返回AM一批NM节点，AM连接节点启动Executor
​		
		5).Executor启动之后，反向注册给Driver,Driver发送task监控task回收结果
​					
​		注意：client模式适用于测试，不适用于生产环境，当在客户端提交多个Application时，会有单节点网卡流量激增问题。
​			在客户端可以看到task的执行和结果。

##### cluster:
​	
	命令：
```shell script
./spark-submit --master yarn-cluster --class .jar ..
./spark-submit --master yarn --deploy-mode cluster --class ..jar ..
```
​					
​	流程：
​		1).在客户端提交Sparkapplication，向ResourceManager申请启动ApplicationMaster
​		
		2).RM收到请求之后，随机找到一台NM节点启动AM
​		
		3).AM启动之后就是Driver，会向RM申请资源用于启动Executor
​		
		4).RM收到请求返回AM一批NM节点，AM连接节点启动Executor
​		
		5).Executor启动之后，反向注册给AM,AM发送task监控task回收结果
​					
### 8.Spark中的术语
​		
		资源层面：
​			Master[ResourceManager] -> Worker[NodeManager] -> Executor -> ThreadPool
​		
		任务层面：
​			Spark Application -> job -> Stage ->tasks
​	
### 9.RDD的宽窄依赖【************************】

* RDD的宽依赖：（shuffle）

​	父RDD 与子RDD partition之间的关系是一对多

* RDD的窄依赖：

​	父RDD 与子RDD partition之间的关系是一对一

​	父RDD 与子RDD partition之间的关系是多对一
​	
### 10.Spark计算模式 - stage【************************】
​		1).stage 是由一组并行的task组成
​		2).Spark的计算模型迭代器计算模型- pipeline管道计算模型
​		3).Stage的并行度由Stage中finalRDD的partition个数决定
​		4).如何提高Stage的并行度？
​			优化
​		5).Stage中的数据何时落地：
​			①.shuffle write
​			②.对RDD进行持久化时


​			
### 11.Spark资源调度和任务调度 【************************】

#### 资源调度：

​			1).集群启动，Worker向Master汇报资源，Master掌握集群资源信息

​			2).spark-submit提交任务，new SparkContext时，创建DAGScheduler和TaskScheduler

​			3).TaskScheduler 向Master申请资源，Master找到满足资源的Worker节点启动Executor

​			4).Executor启动完成之后向Driver注册，Driver掌握一批计算资源

#### 任务调度：
​			
			5).Spark applicatioin 中有一个action时，触发一个job，job中有RDD依赖关系形成的DAG有向无环图

​			6).DAGScheduer按照RDD的宽窄依赖关系切割job划分Stage，将Stage以TaskSet形式提交给TaskScheduler

​			7).TaskScheduler遍历Set获取task，将Task发送task Executor中的ThreadPool中执行

​			8).Driver负责监控task执行，回收结果
​			
#### 注意：
​			1).发送task执行时如果失败，TaskScheduler会重试task，重试3次之后如果依然没有执行成功，由DAGScheduler负责重试task
​				所在的Stage，重试4次，如果失败，stage所在的job就失败了，job失败，job所在的application就失败了。
​				
​			2).TaskScheduler还可以重试执行缓慢的task,推测执行机制，默认是关闭的。如果处理数据很长时间，需要考虑问题：
​				①.是否开启了推测执行 ②.是否遇到了数据倾斜


​			
### 12.粗粒度资源申请和细粒度资源申请  【************************】

#### 粗粒度资源申请：
* application执行之前，首先将所有需要的资源申请完毕，如果没有足够的资源，application处于等待资源充足再执行。
* application执行过程中所有的job使用这批资源，最后一个job执行完成之后，资源才会被释放
* 优点：application执行快
* 缺点：资源不能充分利用
​			

#### 细粒度资源申请：
* application执行时，由各自job申请资源，释放资源，job执行完成之后，当前资源立即释放
* 优点：资源可以充分利用
* 缺点：application相对执行慢


​			
### 13.案例：PVUV案例
​			7 - 1w - 1M
​			
### 14.Spark-submit 提交参数 【************************】
```
--conf
​--class
​--master
​--jars
​--name
​--files
​
--driver-cores
​--driver-memory
​--executor-cores
​--total-executor-cores --standalone 
​--num-executors -- yarn
```

### 15.Spark 源码  
​		1).Master-worker启动
​		2).Spark-submit提交任务-Driver启动
​		3).Driver向Master注册Application申请资源 ，资源调度源码
​			Scheduler()
​		4).任务调度： 从一个action开始任务调度
​		
​		资源调度结论：  【************************】
​			1).Executor是分散在集群中启动的，利于数据处理的本地化
​			2).如果提交任务什么都不指定，集群为当前application 每台Worker启动一个Executor，当前这个Executor使用这台Worker节点的所有core和1G内存
​			3).如果想要在一台节点上启动多个Executor ，需要指定--executor-core
​			4).限制一个application使用的资源，使用--total-executor-cores
​			5).启动Executor不仅和core有关还和内存有关
​			
### 16.二次排序 & 分组取topN 
​		二次排序：自定义对象
​		分组取topN:原生集合排序 | 定长数组


​		
### 17.广播变量 【************************】
​		当Executor中使用到Driver端的变量时，如果不使用广播变量，每个Executor有多少task,在Executor端就有多少变量副本
​		如果使用广播变量在每个Executor中只有一份Driver端变量副本
​		注意：
​			不能将RDD广播出去，可以将RDD回收到Driver之后将结果广播出去
​			广播变量只能在Driver端定义
​			
### 18.累加器  【************************】
​		累加器相当集群中统筹的变量，负责进行分布式累加。
​		累加器注意：
​			累加器在Driver端定义，在Executor端累加
​			
​		自定义累加器：
​			需要继承 AccumulatorV2 [Info,Info]实现方法：reset() ,copy(),isZero,add(),merge(),value


​		
### 19.WEBUI
​		1).job ->stage ->task
​		2).如何查看数据倾斜？
​		3)如何数据本地化级别？
​		4).如何查看gc数据量？
​		5).搭建历史日志服务器
​		
### 20.针对Standalone 的Master HA搭建 
​		利用zookeeper搭建
​		
### 21.Spark Shuffle 【************************】
​
#### HashShuffleManager

###### 普通机制
​	
	产生磁盘小文件个数：M*R
​	
	问题： 落地磁盘数据多，shuffle数据量大，拉取的数据量大，网络之间创建拉取数据的连接多。
​				

###### 优化机制
​	产生的磁盘小文件个数：E*R
​		
	问题：相对于普通机制大大减少磁盘小文件个数，但是当reduce task多的时候，产生的磁盘小文件个数也是非常多。
​				

#### SortShuffleManager [解决磁盘IO的问题]
​			
##### 普通机制：
​		
	产生磁盘小文件个数：2*M
​		
	过程：
​		1).map task处理数据，首先将数据写往一个5M数据结构
​			
		2).sortshuffle有估算机制，估算本次内存不足时，申请内存 = 2*估算 -当前
​						如果申请到内存，继续写往内存，申请不到，溢写磁盘
​		3).多次溢写磁盘的文件会合并，合并过程中会有排序
​			
		4).最终数据合并成一个数据文件和一个索引文件
​					
##### bypass机制：
​		
	产生磁盘小文件个数：2*M
​		
	过程：与普通机制一样，只是没有落地多个文件，而是线性写往一个磁盘文件，没有排序
​		
	bypass机制使用的条件：
​		1).算子不能有map端的预聚合
​			
		2).reduce task个数必须小于200 [spark.shuffle.sort.bypassMergeThreshold]
​		

### 22.Shuffle文件寻址 【************************】
​
#### 对象：
​
##### MapOutputTracker:
​		主从关系
​		
		MapOutputTrackerMaster[Driver]
​		
		MapOutputTrackerWorker[Executor]

##### BlockManager:
​		`BlockManagerMaster`[Driver]
​			
			`DiskStore`:负责磁盘数据
​			
			`MemoryStore`:负责内存数据
​			
			`BlockTransferServer`:负责拉取数据

		`BlockManagerSlave`[Executor]
​			`DiskStore`:负责磁盘数据
​			
			`MemoryStore`:负责内存数据
​			
			`BlockTransferServer`:负责拉取数据

​					
##### 小文件寻址过程：
* 1).map task处理完数据之后，将数据位置信息封装到一个mapStatus对象中，通过mapoutTracker汇报给Driver
​				Driver掌握了落地数据小文件位置
* 2).reduce task 处理数据之前首先向Driver获取磁盘小文件位置信息，通过BlockManager连接数据所在的节点
* 3).连接上之后，通过BlockTransferService 启动5个线程拉取数据，一次最多拉取48M
* 4).拉取过来的数据存储在Executor Shuffle内存中，内存不足还会溢写磁盘。


​		
### 23.Spark 内存管理

#### 静态内存管理[Spark1.2之前使用，目前使用的是统一内存]：
​			0.2:task运行
​			0.2：spark.shuffle.memoryFraction
​				0.2：预留内存
​				0.8：Shuffle聚合内存
​			0.6：spark.stroage.memoryFraction
​				0.1：预留内存
​				0.9
​					0.2：反序列化内存 spark.storage.unrollFraction
​					0.8：RDD的缓存和广播变量
​					
#### 统一内存管理：【************************】
​			300M:预留
​			（总-300M）：
​				0.6：spark.memory.fraction
​					0.5:RDD的缓存和广播变量 spark.memory.storageFraction
​					0.5：shuffle聚合内存
​				0.4：task运行


​		
### 24.Shuffle参数 【************************】
​	`spark.reducer.maxSizeInFlight`：48M
​	`spark.shuffle.io.maxRetries` ：3 
​	`spark.shuffle.io.retryWait` ：5s
​	`spark.shuffle.sort.bypassMergeThreshold` 200


​		
### 25.Spark SQL
​		SparkSQL 支持写SQL对分布式的数据进行查询，底层操作的对象是Dataset,对数据进行分析时可以使用Dataset API 
​		也可以使用将Dataset转成临时视图方式写SQL语句查询
​		
### 26.SparkSQL 过程
​		Hive -> Shark -> SparkSQL
​		
​		Hive on Spark--Shark
​			Hive: 解析优化，存储
​			Spark：执行引擎
​		Spark on Hive--SparkSQL
​			Hive:存储
​			Spark:解析优化，执行引擎
​			
### 27.HQL&Shark & SparkSQL 【************************】
​		1).HQL就是Hive SQL，底层解析成MR job运行
​		2).Shark 采用Hive进行解析优化SQL,底层使用Spark执行
​		3).SparkSQL底层解析优化使用Spark,底层使用Spark执行
​		4).Shark兼容Hive语法，相对于HQL速度快，查询数据来源Hive,用于与Hive耦合度高Shark被停用，出来SparkSQL
​		5).SparkSQL 兼容Hive和Shark语法，查询数据不仅可以来源Hive还可以对原生的RDD数据进行使用SQL查询。
​		6).SparkSQL 2.0+采用Dataset数据结构，速度更快，还可以将查询的结果转换成RDD使用
​		
### 28.RDD &Dataset & DataFrame 【************************】
​		1).RDD是Spark Core底层操作数据结构，是分布式数据集，有分区概念
​		2).Dataset是SparkSQL底层操作的数据结构，也是分布式数据集，也有分区概念，在Shuffle操作中，可以不用将序列化
​			后的数据重新转换成对象可以调用hash、sort等方法，性能比RDD好。
​		3).DataFrame = Dataset[Row] ,如果想要在SparkSQL中写SQL语句查询数据，必须将数据转换成DataFrame注册临时视图
​		
### 29.SparkSession & SparkConf & SparkContext & SQLContext 【************************】
​		1).SparkConf 在Spark2.0 之前使用的对象，在SparkCore、SparkSQL、SparkStreaming中都有使用，设置Spark application 
​			配置参数。在Spark2.0 之后，SparkConf 可以由SparkSession替代，SparkSession包含了SparkConf
​		2).SparkContext 在Spark2.0 之前使用的对象，在SparkCore、SparkSQL、SparkStreaming中都有使用，设置Spark application 
​			入口。在Spark2.0 之后，SparkContext 可以由SparkSession替代，SparkSession包含了SparkContext
​		3).SQLContext 是在Spark2.0之前创建DataFrame的对象，在Spark2.0+之后，被封装到SparkSession中
​		4).SparkSession是Spark2.0+ 之后的对象，包含了SparkConf,SparkContext,SQLContext但是不包含StreamingContext
​	
### 30.	创建DataFrame的方式
​		1).json数据转换DataFrame
​		2).读取parquet格式数据
​		3).读取原生RDD的数据： 动态创建Schema & 反射的方式
​		4).读取MySQL中的数据加载DataFrame
​		5).读取Hive中的数据加载DataFrame
​			配置Spark On Hive 
​			
### 31.Spark on Hive配置
​		在提交Spark任务的客户端$SPARK_HOME/conf/配置hive-site.xml，连接原数据
​		
### 32.Spark UDF & UDAF 【************************】
​	UDF:自定义函数
​	
​	UDAF:自定义聚合函数
​		`initialize`() /`update`() /`merge`()
​			
### 33.Spark 开窗函数 【************************】
​	row_number() over(partition by  xxx order by xx ) as rank
​	
### 34.SparkStreaming & Storm  【*】

* 1).`Strom`是纯实时处理数据，`SparkStreaming`微批处理数据，`SparkStreaming`吞吐量大

* 2).`SparkStreaming`擅长处理复杂的流场景业务，`Storm`擅长处理实时场景业务

* 3).`Strom`的事务相对于`SparkStremaing`完善，`SparkStreaming`手动维护保证数据处理的事务

* 4).`Strom`支持动态资源调度，Spark1.2之后也是支持，不建议开启

​		
### 35.SparkStreaming处理Socket数据流程
* 1).`SparkStreaming`启动之后，首先启动一个job接收数据，接收来的数据每隔`batchInterval`封装一个batch中,batch又被封装到一个
	​			RDD中，RDD又被封装到`DStream`，`parkStreaming`底层操作的就是`DStream`
	​			
* 2).生成`DStream`之后，需要启动新的job去处理`DStream`的数据，在本地运行需要设置 `local[2]`

* 3).处理`DStream`时，`DStream`有自己的Transformation类算子，懒执行，需要`DStream`的`outputOperator`类算子触发执行

* 4). `batchInterval` 大于集群处理一批次数据时间，集群资源不能充分利用

​			`batchInterval` 小于集群处理一批次数据时间，集群有任务堆积，最好是batchInterval与处理一批次数据的时间相同。

* 5).如何调节`BatchInterval` ? 参照webui去调节
	​		

### 36.SparkStreaming 算子

#### Transformation类算子：
* map,filter,flatMap...

* updateStateByKey，window,reduceByKeyAndWindow,Transform

#### outputOperator类算子：
* print，foreachRDD,saveAsTextFile
	​			

### 37.Driver HA

#### 代码

val StreamingContext = StreamingContextFactory.getOrCreate(ckpointdir,fun(...))


#### 设置的checkpoint中可以存储：

​1).conf配置

​2).DStream逻辑

​3).数据批次处理位置

#### 问题：

​1).从checkpoint中获取停掉SparkStreaming之前的位置时，会有数据重复处理。

​2).代码逻辑改变时，不能使用这种方式。
​			
### 38.Kafka 【************************】

#### Kafka是什么

分布式的消息系统，数据默认存储在磁盘上，默认保存7天。

#### Kafka的角色：

##### producer:

消息生产者，两种生产方式：

1).轮询  

2).基于Key的hash

##### broker:

组成Kafka集群的节点，之间是没有主从关系，依赖zookeeper协调，broker负责消息的读写和存储，一个broker

​可以管理多个partition


##### topic:

消息队列，一类消息。topic是由partition组成，创建的时候可以指定。

##### partition:

​* 组成topic的单元，每个partition对应broker中的目录

​* partition有副本，可以在创建topic时指定。副本就是为了保证高可用的。

​* 每个partition内部消息是严格FIFO。

##### onsumer:
​* 消息的消费者，每个consumer都有自己的消费者组

​* 同一个消费者组内的消费者消费同一个topic时，这个topic中相同的数据只能被消费一次

​* 不同的消费者组之间消费同一个topic时，互不影响。
​				
##### zookeeper:

​* 负责存储原数据，broker，topic,partiton,删除topic，topic配置

​* 在kafka0.8.2 版本之前，zookeeper可以存储消费者的offset

​				
#### kafka命令：

* 创建topic

* console当做生产者

* console当做消费者

* 查看topic的详细信息

* 删除topic
	​			

#### Kafka Leader均衡机制：
保证Kakfa集群负载均衡。
​		
​		
### 39.SparkStreaming + Kafka整合管理offset 【************************】

#### SparkStreaming1.6 + Kakfa0.8.2：

##### Receiver:
* 1).receiver模式采用了Receiver接收器模式，每批次将数据接收到Spark中，接收数据存储级别MEMORY_AND_DISK_SER_2

* 2).利用zookeeper管理消费者offset

* 3).receiver模式当Driver挂掉时，有可能导致数据丢失，开启WAL机制来避免数据丢失

* 4).开启WAL机制之后，将数据不仅备份到其他节点，也会备份到指定checkpoint目录中[HDFS]

* 5).开启WAL需要设置checkpoint。

* 6).开启WAL注意问题：①.数据处理延迟加大 ②.可能将接收来的数据存储级别降级 ③.重复处理数据问题

* 7).Receiver模式读取Kafka数据的实现api是High Level Consumer api ,这种api不支持获取每批次的offset，对于需要精准维护offset
  ​					的场景，receiver模式不适用。

* 8).receiver模式并行度 `spark.streaming.blockInterval = 200ms`，提高并行度时可以减少这个参数，最低不能低于50ms

  

##### Direct:

  * 1).Direct模式没有采用Receiver接收器模式，采用直连方式，用到每批次数据时，直接获取
  * 2).Direct模式使用Spark自己维护消费者offset，需要设置checkpoint
  * 3).Direct模式并行度与读取的topic的分区是一一对应的
  * 4).Direct模式底层读取Kafka消息的api是Sample Consumer api，支持获取每批次offset，对于需要精准维护消费者offset场景可以使用使用手动维护。
    ​					

#### SparkStreaming2.0 + Kafka0.10+：

* 由于Kafka版本升级，默认不使用高阶和低级 api消费数据，导致SparkStreaming读取Kafka数据代码变化。采用新的api来整合。
* 同时也丢弃了receiver模式，在Spark2.0之后匹配的Kafka版本是Kafka0.10+ ，只有Direct模式，这里的Direct模式实现
* 方式是使用new Consumer api实现。
* 默认使用了Kafka维护消费者offset，也可以使用Spark checkpoint维护，也可以手动维护
	spark Checkpoint维护offset:
	​	1).代码改变不能使用

​		
​		2).重复处理数据问题

​	Kafka维护offset
​		
​		1).设置自动提交`enable.auto.commit`为true，相当于最多消费一次数据，有可能丢失数据
​		
​	   	2).设置自动提交`enable.auto.commit`为false,需要保证处理完当前批次数据后，手动异步的提交消费者offset.
​			存在Kafka中的offset，在Kafka停机超过24小时，就会被清空
​					
​		3).手动维护消费者offset
​						可以从每一批次数据中获取消费者offset，也可以获取每条数据的offset,可以手动维护

* Spark读取Kafka数据消费数据策略:
    ​`LocationStrategies.PreferConsistent`:均匀在每个Executor中处理数据
    ​`LocationStrategies.PreferBrokers`： Executor处理数据来源于当前节点的broker
    ​`LocationStrategies.PreferFixed`：可以指定哪些数据去往哪些节点处理
    ​				 
### 40.Kafka 和SparkStreaming参数：【*】

#### Kafka参数: 

```
bootstrap.servers
key.deserializer
value.deserializer
group.id
auto.offset.reset
enable.auto.commit
heartbeat.interval.ms  3s
session.timeout.ms 	30s
group.min.session.timeout.ms 6s
group.max.session.timeout.ms 300
```

##### SparkSteaming参数：

```
spark.streaming.receiver.writeAheadLog.enable
spark.streaming.blockInterval
spark.streaming.backpressure.enabled 
spark.streaming.backpressure.initrate
spark.streaming.receiver.maxRate 
```



## **Spark优化：(重点)**

### 1.分配更多的资源  

#### 搭建集群：yarn 总就是给NodeManager分配的core和内存

#### 提交任务：
>  ./spark-submit --master --conf xxx = xx --executor-cores --class ..jar ...

##### 指定参数：

```
--driver-cores
--driver-memory
--executor-cores
--executor-memory
--total-executor-cores --standalone 
--num-executors -- yarn
```


##### 分配资源如何计算? 
	一个core可以处理2~3个task,这样利于资源利用。


### 2.增加并行度
1).sc.textFile(num)

2).sc.makeRDD(num)

3).sc.parallelizePairs(num)

4).sc.parallelize(num)

5).reduceByKey(num)|join(num)|distinct(num)

6).repartition(num)|coalesce(num)

7).spark.default.parallelism

8).spark.sql.shuffle.partitons

9).自定义分区器

10).SparkSteaming : 
	①.receiver ： spark.streaming.blockInterval 
	②.Direct模式 ： 与读取的topic的分区数一致
		
### 3.代码优化

1).避免重复的RDD

2).对经常使用的RDD进行持久化

3).尽量避免使用shuffle类算子 ： map + 广播变量 代替join

4).使用map端预聚合的算子：

	reduceByKey,aggtegateByKey,CombineByKey

5).使用高性能算子

	mapPartition代替map
	
	foreachPartition代替foreach

6).使用广播变量

7).优化数据结构

	尽量使用原生数据类型代替字符串|尽量使用字符串代替对象 | 进行使用数组代替集合

8).使用kryo序列化
		
### 4.数据本地化设置

#### 数据本地化级别：
	Process_local
	node_local
	no_pref:
	rack_local
	any:

#### 数据本地化调节：
	spark.locality.wait.process
	spark.locality.wait.node
	spark.locality.wait.rack

### 5.内存优化
主要保证task的计算运行内存
		
### 6.堆外内存优化

* 节点之间的连接时长：spark.network.timeout 120s 

* 每个executor的堆外内存：
			spark.executor.memoryOverhead
			
### 7.shuffle参数优化

### 8.数据倾斜的处理

1). 使用Hive etl预处理

2). 过滤少数倾斜的key

3). 增加并行度

4). 双重聚合

5). reduce join 转换map join

6). 采样倾斜的key分拆join   【*】

7). 膨胀加前缀处理倾斜

## 项目：

### 1.数据处理流程 -- lambda架构

* 批处理
	
* 流式处理
		

### 2.数据来源，数据量

### 3.集群分布

### 4.代码

## 机器学习：

### 1. pyspark

* Python环境搭建 使用Anconda安装python3.6 

* PySpark环境配置 ：Spark1.6 + python2.7+ 最高版本是pytho3.5
	​				   Spark2.0+ python3.5+
	​		

* 开发工具配置
		
### 2.线性回归算法  
* 1). 线性回归算法有监督的值预测模型。

* 2). 线性回归算法的公式：y = w0+w1x1+w2x2+w3x3+....+wnxn

* 3). y ： 预测值|真实值 | x :特征，维度 | w0 :截距 | w... : 权重

* 4). 线性回归误差公式：error 

* 5). 根据误差公式求导梯度下降的方式得到一组w的值

* 6). w值确定，模型就确定，给定一个样本点（含特征x） 可以带入模型得到对应的y值（预测值）

* 7). 什么是过拟合欠拟合问题。解决过拟合和欠拟合问题[减少特征，增加数据量，调节步长]

* 8). 调节步长，步长不要太大，也不能太小，不要大于1,0.0001 以三倍速度增大或者减少

* 9). 只要涉及到值预测都可以选择使用。
		
### 3.贝叶斯算法  【************************】

* 1).贝叶斯算法是基于概率的有监督的分类算法

* 2).贝叶斯公式：P(A|B)*P(B) = P(B/A)*P(A)

* 3).在给定的条件下根据贝叶斯算法计算出来当前条件下属于哪一类的概率大，当前条件下的样本就属于哪一类。

* 4).贝叶斯在多条件下的推广使用，多个条件必须是独立条件

* 5).贝叶斯拉普拉斯估计：保证每个特征在每个类别中出现的次数非零，默认加的值是1

* 6).垃圾邮件分类
	
### 4.KNN算法
* 1).KNN最近邻基于距离的有监督分类算法，常用于文本分类

* 2).KNN分类步骤：

	A.找出距离当前样本点最近的K个邻居 -- 基于欧式距离计算

	B.看下最近的K个邻居中大多数属于哪一类，当前样本就属于哪一类

* 3).K值的选择：一般不超过20

* 4).KNN问题：当样本容量不均匀时，很容易将样本分到大样本类别中，保证数据类别均匀。

* 5).KNN数据分类案例+KNN相亲案例

### 5.KMeans 【************************】
* 1).KMeans是无监督的聚类算法

* 2).KMeans聚类过程：

    A. 随机在范围内找到K个中心点

    B. 计算所有样本点到当前中心点的距离，每个样本点距离哪个中心点距离近，就属于哪个类别

    C. 所有样本点经过一轮计算之后，重新根据均值方式确定每个类别的中心点

    D. 重复B~C,最终达到每个类别中的中心点不再变化位置，聚类完成。
    
* 3).聚类的好坏？聚类出的每个类别之间差异非常大，每个类内部的样本点非常相似，则聚类越好

* 4).如何确定K？①.已知类别 ②.根据肘部法确定K的值。

* 5).开始得到K个随机中心点时，如果K个点距离非常接近，训练得到模型最终结果时间长。

### 6.KMeans++

* 1).对KMeans算法的优化，在随机得到K个中心点时，规则：获取下一个中心点时，距离之前中心点的距离越远越好，这样
		达到目的是训练模型迭代次数少。

* 2).微博聚类案例

### 7.TF-IDF

* 找出文本关键词

* 公式：
```
TF-IDF = TF*IDF
```

### 8.逻辑回归算法  【************************】
* 1).线性有监督的分类模型
* 2).逻辑回归公式 ？？f(z) = .... ; z就是多元线性回归公式

* 3).f(z)的图像是一个S型图像，逻辑函数也叫S型函数
* 4).逻辑回归中f(z)的值默认的分类阈值是0.5,0.5-1是一类，0-0.5是一类，如果f(z)=0.5 一般归为下分类。
* 5).逻辑回归损失函数推导：
    根据最大似然估计思想推导出逻辑回归的损失函数
    		对损失函数使用梯度下降的方式得到一组合适的w值

### 9.逻辑回归优化 【************************】
* 1).有无截距

* 2).对于线性不可分问题增加维度

* 3).调整分类阈值

* 4).鲁棒性调优-抗干扰调优 L1&L2正则化

	L1和L2正则化使模型w的值处于较小的水平上，但不出现过拟合现象

	L1降维

* 5).数据归一化

	最大最小值归一化

	方差归一化
	
	均值归一化

### 10.ROC & AUC 【************************】
* 混淆矩阵
* TP/P : 召回率
* TP/Y : 正确率
* TP+TN/p+n : 准确率
	
* ROC:以FPR为横轴，以TPR为纵轴画出的曲线

	如果ROC曲线是通过（0,0）和（1,1）点的一个直线，这个模型随机分类模型

* AUC:

	AUC以公式的方式表示了ROC曲线

	AUC =0.5 随机分类

	0<AUC<0.5 不如随机分类

	0.5<AUC<1 优于随机分类

### 11.APP推荐

#### 1. 协同过滤思想：

* 基于用户的协同过滤思想

* 基于商品的协同过滤思想
			
#### 2. App推荐就是基于商品的协同过滤思想+隐式数据反馈



#### 3. 推荐系统架构：

* 离线训练模型

* 在线使用模型 -dubbo​	

#### 4. 推荐系统实现过程：

##### 4.1 数据：
* app基本信息表

* 用户下载app表

* 用户浏览app表

##### 4.2 构建逻辑回归模型：
* 用户浏览app，我们认为当前用户对这个app感兴趣，用户历史下载就是反映用户特点的地方，根据用户浏览的app与
	​		
* 用户历史下载的app构建关联特征，用户浏览app是否下载就是二分类问题。根据逻辑回归模型得到两两app的关联重要程度
	​		
* 获取每个特征的w值当做两个app关联重要程度保存模型文件。


​			
##### 4.3 如何给用户进行推荐?
* 首先获取所有可推荐的app，计算每个app值得推荐的分值，对分值排序获取topN.
	​				

* 计算分值：获取当前app与当前用户历史下载中的app关联特征 ，从模型文件中得到关联特征的值累加得到总分值

##### 4.4 冷启动问题：
​	新用户登录没有历史下载，无法做出推荐列表
​				
##### 4.5 解决冷启动问题：

* 1).根据自己的方式
	​		
* 2).在构建模型时，加入基本特征

​	

## Flink:

### 1. Flink

​	Flink是计算框架，包含批处理，流处理,SQL处理，基于事件驱动的计算框架，节点之间传递消息是以流的方式传递

### ​2. MR & Strom & Flink & Spark 【*】

* MR:只有批处理

* Strom:纯实时处理数据，吞吐量低

* Flink：含有批处理，SQL处理，纯实时流处理计算框架

* Spark:含有批处理，SQL处理，流处理的计算框架
	​		

### 3. Flink架构 【*】
1).处理数据流程：Source ->Transformation -> Sink

2).有界数据流和无界数据流

3).批处理底层是DataSet,流操作对象DataStream

4).jobManger负责task发送，与TaskManager通信，负责checkpoint

5).taskManager负责运行task,taskManager有taskSlot,taskSlot是隔离task执行内存

6).每个节点的TaskSlot决定了当前节点的并行度，最好与当前节点的core的个数一致

7).Flink中的累加器

8).Flink不是K,V格式编程模型，需要指定虚拟的key。批：groupBy 流：keyBy

9).Flink自定义dataSource

10).Flink自定义DataSink


### 4. Flink搭建

* 基于Flink-Standalone

* 基于Flink-Yarn


### 5. Flink提交任务

#### standalone:

```shell script
./flink run -c xxx ...jar ...
```
​
#### yarn:

```shell script
./flink run -m yarn-cluster -c xxx xxx.jar xxx
```



### 6.Flink +Kafka 整合 【*】

​	两阶段提交保证offset

## Azkaban:

​	任务流调度引擎

## Kylin:

### ​1).KyLin是MOLAP分析工具，可以对海量数据达到亚秒级回应



### 2).OLTP & OLAP区别



### 3).OLAP概念、操作、分类、特点



### 4).KyLin原理： 【*】

​	A. 对数据进行维度划分，构建cube
​	B. 抽取Hive中的数据，使用Spark ||MR预计算，将结果存在HBase
​	C. 客户端查询直接是在HBase中获取数据
​	

###  5).Kylin 安装& 集群安装 : 环境和硬件要求



###  6).Kylin配置



###  7).Kylin概念：	
* 事实表
* 维度表
* 事实
* 维度
* cube
* cuboid
* 星型模型
* 雪花模型

###  8).webui构建cube: 

A.度量必须来源于事实表

B.维度中如果可以有事实表中的某个字段推断出来可以在维表中不参与构建cubie的计算

###  9).cube运行过程 【*】

###  10).cube的算法：【*】

A . layered cubing 
	
B. Fast cubing 

###  11).Kylin案例



## CDH:	

### 	hue:

​		对大数据组件的包装

### 	impala:

​		impala原理
​		基于纯内存计算返回结果
​		impala&Hive区别？

### 	oozie:

​		任务流调度系统，需要配置xml，转换成MRjob执行
