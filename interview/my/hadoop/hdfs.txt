hbase读取hdfs的方式  hive读取hdfs的方式

1) HDFS的 读写 流程?hdfs了解多少？读写流程，架构
读:
  a.client用FileSystem的open()函数打开文件（创建path）
  b.DistributedFileSystem查找namenode结点元数据信息，将元数据信息封装在输入流中返回给客户端。
  c.client用返回的输入流读取此文件第一个数据块的最近的数据节点（就近原则，优先读取同机 架的 block）
  d.当此数据块读取完毕时，DFSInputStream 关闭和此 数据节点的连接，然后连接此文件下一个数据块的最近的数据节点。
  e.当客户端读取完毕数据的时候，调用 FSDataInputStream 的 close 函数。 
  f.在读取数据的过程中，如果客户端在与数据节点通信出现错误，则尝试连接包含此数据块的下一个数据节点。
    失败的数据节点将被记录，以后不再连接。
  g.客户端和 NN 获取一部分 Block（获取部分 block 信息，而不是整个文件全部的 block 信息，读完这部分 block 后，
    再获取另一个部分 block 的信息）副本位置列表，线性地和 DN 获取 Block，最终合并为一个文件，在 Block 副本列表中按距离
    择优选取。
写:
  a.客户端创建path和DistributedFileSystem 对象. 
  b.DistributedFileSystem 对象调用元数据节点，在文件系统的命名空间中创建一个新的文件，元数据节点首先确定文件原来不存在，
    并且客户端有创建文件的权限，然后创建新文件，并标识为“上传中”状态，即可以看见，但不能使用。
  c.DistributedFileSystem 返回 DFSOutputStream，客户端用于写数据。 
  d.客户端开始写入数据，DFSOutputStream 将数据分成块，写入 data queue（Data queue 由 Data Streamer 读取），
    并通知元数据节点分配数据节点，用来存储数据块（每块默认复制 3 块）。分配的数据节点放在一个 pipeline 里。
    Data Streamer 将数据块写入 pipeline 中的第一个数据节点。第一个数据节点将数据块发送给第二个数据节点。
    第二个数据节点将数据发送给第三个数据节点。注意：并不是第一个数据节点完全接收完 block 后再发送给后面的数据节点，
    而是接收到一部分就发送，所以三个节点几乎是同时接收到完整的 block 的。DFSOutputStream 为发出去的数据块保存了ack queue，
    等待pipeline 中的数据节点告知数据已经写入成 功。如果 block 在某个节点的写入的过程中失败：关闭 pipeline，
    将 ack queue 放至 data queue 的开始。已经写入节点中的那些 block 部分会被元数据节点赋予新的标示，
    发生错误的节点重启后能够察觉其数据块是过时的，会被删除。失败的 节点从 pipeline 中移除，block 的其他副本则写入
    pipeline 中的另外两个数据节点。元数据节点则被通知此 block 的副本不足，将来会再创建第三份备份。 
  e.ack queue 返回成功。 
  f.客户端结束写入数据，则调用 stream 的 close 函数，最后通知元数据节点写入完毕 
  g.客户端切分文件Block，按 Block 线性地和 NN 获取 DN 列表（副本数），验证 DN 列表后以更小的单位流式传输数据，
    各节点两两通信确定可用，Block 传输结束后，DN 向 NN 汇报 Block 信息，DN 向 Client 汇报完成，Client 向 NN 汇报完成，
    获取下一个 Block 存放的 DN 列表，最终 Client 汇报完成，NN 会在写流程更新文件状态。

2) 写流程中备份三，其中一个写失败了怎么弄的？(如果一个节点或多个节点写入失败)
只要成功写入的节点数量达到`dfs.replication.min`(默认为1)，那么就任务是写成功的。
然后NameNode会通过异步的方式将block复制到其他节点，使数据副本达到`dfs.replication`参数配置的个数

3) hdfs HA（过程，启动流程）
①开启zookeeper服务

　　$>zkServer.sh start

②开启`journalNode`守护进程（在`journal`协议指定的节点上执行）[ˈdʒɜːnl]

　　$>hadoop-daemon.sh start journalnode

③开启namenode守护进程（在nn1和nn2执行）

　　$>hadoop-daemon.sh start namenode

④开启datanode守护进程

　　$>hadoop-daemons.sh start datanode（在namenode节点上执行开启全部datanode）

⑤开启zkfc守护进程

　　$>hadoop-daemon.sh --script $HADOOP_PREFIX/bin/hdfs start zkfc　