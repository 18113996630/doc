#1、用户行为日志分析(离线数据分析)

##Overview:
日志生成渠道:Nginx、Ajax(页面点击位置)
用户行为日志内容:一次访问的内容(ip 账号 访问的时间和区域 访问的所使用的客户端 访问的模块Id跳转的链接地址)
1) 访问的系统属性:操作系统、浏览器等待
2) 访问特征:点击的url、跳转的链接地址(referer)、页面停留地址
3）访问信息：session_id、访问ip(访问城市)等

####意义
网站的眼睛
网站的神经
网站的大脑

##离线数据架构

###离线数据处理流程:
1) 数据采集 Flume:web日志写入到HDFS
	流处理框架:flume+kafka+Storm
2) 数据清洗
	脏数据
	Spark、Hive、MapReduce或者其他分布式计算框架
	清洗完之后的数据可以存放在HDFS(Hive/Spark SQL) 
3) 数据处理
	按照需求进行相应业务的统计的分析 
	Spark、Hive、MapReduce或者其他分布式计算框架
4) 处理结果入库
	结果存放到RDBMS、NOSQL
5) 数据的可视化
	通过图形化展示的方式展现出来:饼图、柱状图、地图、折线图
	Echarts、HUE、Zeppelin

###数据清洗
####技术:Spark SQL	
####功能实现:
1. 使用Spark SQL解析访问日志
2. 解析出课程编号、类型
3. 根据ip解析出城市信息(github ipdatabase)
4. 使用Spark SQL将访问时间按照天进行分区
一般的日志的访问时间进行相应的分区，比如d,h,m5(每5分钟一个分区)

####离线数据处理架构:
		在线处理 Spark/Storm->搜索引擎->机器学习等
日志 			           ->数据输出         
			
		离线处理 Flume->HDFS->数据清洗存放入HDFS->Hive->RDBMS->数据输出	
						作业调度:Oozie/Azkaban
###项目需求: