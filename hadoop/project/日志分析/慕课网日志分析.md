# Spark Streaming 实战 日志分析

[(一)数据清洗+统计访问](https://blog.csdn.net/lihaogn/article/details/82461601)

[(二)数据可视化](https://blog.csdn.net/lihaogn/article/details/82469193)

## （一）数据清洗+统计访问量
### 1 项目需求
#### 1）需求
统计今天到目前为止的访问量
统计今天到目前为止从搜索引擎过来的课程的访问量

#### 2）开发环境与技术选型
* IDEA+maven
* flume+kafka+HBase

#### 3）安装配置 HBase
##### 下载、解压、配置环境变量
##### 配置文件
conf/hbase-env.sh

修改JAVA_HOME
export HBASE_MANAGES_ZK=false

conf/hbase-site.xml
```xml
<configuration>
    <property>
        <name>hbase.rootdir</name>
         <value>hdfs://hadoop000:8020/hbase</value>
    </property>
    <property>
        <name>hbase.cluster.distributed</name>
        <value>true</value>
    </property>
    <property>
        <name>hbase.zookeeper.quorum</name>
        <value>hadoop000:2181</value>
    </property> 
</configuration>
```
conf/regionservers
```
localhost1
```
4）HBase 建表
```
// 1 启动hbase
start-hbase.sh
// 2 启动shell
hbase shell
// 3 建表
create 'course_clickcount','info'
create 'course_search_clickcount','info'
// 4 查看数据表
list 'course_clickcount'
// 5 查看数据表信息
describe 'course_clickcount'
// 6 查看表数据
scan 'course_clickcount'
```
5）
    (代码地址)[https://github.com/lihaogm/SparkStreamTrain]

### 2 模拟日志生成

1）使用python开发日志生成器模拟产生日志，每分钟产生一次日志信息

generate_log.py
```
#coding=UTF-8

import random
import time

url_paths=[
    "class/112.html",
    "class/128.html",
    "class/145.html",
    "class/130.html",
    "class/146.html",
    "class/131.html",
    "learn/821",
    "course/list"
]

ip_slices=[132,156,124,10,29,167,143,187,30,46,55,63,72,87,98,168]

http_referers=[
    "https://www.baidu.com/s?wd={query}",
    "https://www.sogou.com/web?query={query}",
    "https://cn.bing.com/search?q={query}",
    "https://www.so.com/s?q={query}"
]

search_keyword=[
    "spark sql实战",
    "hadoop 基础",
    "storm实战",
    "spark streaming实战"
]

status_code=["200","404","500"]

def sample_status_code():
    return random.sample(status_code,1)[0]

def sample_referer():
    if random.uniform(0,1)>0.2:
        return "-"
    refer_str=random.sample(http_referers,1)
    query_str=random.sample(search_keyword,1)
    return refer_str[0].format(query=query_str[0])

def sample_url():
    return random.sample(url_paths,1)[0]

def sample_ip():
    slice=random.sample(ip_slices,4)
    return ".".join([str(item) for item in slice])

def generate_log(count=10):
    time_str=time.strftime("%Y-%m-%d %H:%M:%S",time.localtime())

    f=open("/home/hadoop/data/streaming_access.log","w+")

    while count >=1:
        query_log="{ip}\t{local_time}\t\"GET /{url} HTTP/1.1\"\t{status_code}\t{refer}".format(url=sample_url(),ip=sample_ip(),refer=sample_referer(),status_code=sample_status_code(),local_time=time_str)
        print(query_log)
        f.write(query_log+"\n")
        count=count-1

if __name__ == '__main__':
    # 每一分钟生成一次日志信息
    while True:
        generate_log()
        time.sleep(60)
```

修改日志生成的位置 def generate_log

>> python generate_log.py

### 3 flume收集日志并对接kafka

1）编写flume配置文件，streaming_project2.conf
```
exec-memory-kafka.sources= exec-source
exec-memory-kafka.sinks=kafka-sink
exec-memory-kafka.channels= memory-channel

exec-memory-kafka.sources.exec-source.type=exec
exec-memory-kafka.sources.exec-source.command=tail -F /home/hadoop/data/streaming_access.log
exec-memory-kafka.sources.exec-source.shell= /bin/sh -c

exec-memory-kafka.memory-channel.type=memory

exec-memory-kafka.sinks.kafka-sink.type = org.apache.flume.sink.kafka.KafkaSink
exec-memory-kafka.sinks.kafka-sink.brokerList=hadoop000:9092
exec-memory-kafka.sinks.kafka-sink.topic=test_topic
exec-memory-kafka.sinks.kafka-sink.batchSize=5
exec-memory-kafka.sinks.kafka-sink.requireedAcks=1

exec-memory-kafka.sources.exec-source.channels=memory-channel
exec-memory-kafka.sinks.kafka-sink.channel= memory-channel
```

### 4 业务开发

4.1 消费kafka数据、数据清洗与统计

1）实体类

ClickLog.scala
```
package com.lihaogn.sparkProject.domain

/**
  * 清洗后的日志格式
  *
  * @param ip
  * @param time
  * @param courseId
  * @param statusCode 日志访问状态码
  * @param referer
  */
case class ClickLog(ip: String, time: String, courseId: Int, statusCode: Int, referer: String)
```

CourseClickCount.scala
```
package com.lihaogn.sparkProject.domain

/**
  * 课程点击次数实体类
  *
  * @param day_course  对应HBase中的rowkey
  * @param click_count 访问次数
  */
case class CourseClickCount(day_course: String, click_count: Long)
```

CourseSearchClickCount.scala
```
package com.lihaogn.sparkProject.domain

/**
  * 从搜索引擎过来的课程点击数实体类
  * @param day_search_course
  * @param click_count
  */
case class CourseSearchClickCount(day_search_course: String, click_count: Long)
```

2）工具类

DateUtils.scala
```
package com.lihaogn.sparkProject.utils

import java.util.Date

import org.apache.commons.lang3.time.FastDateFormat

/**
  * 日期时间工具类
  */
object DateUtils {

  val OLD_FORMAT = FastDateFormat.getInstance("yyyy-MM-dd HH:mm:ss")

  val TARGET_FORMAT = FastDateFormat.getInstance("yyyyMMddHHmmss")

  def getTime(time: String) = {
    OLD_FORMAT.parse(time).getTime
  }

  def parseToMinute(time: String) = {
    TARGET_FORMAT.format(new Date(getTime(time)))
  }

  def main(args: Array[String]): Unit = {
    println(parseToMinute("2018-9-6 13:58:01"))
  }
}
```

添加依赖
```xml
<!-- cloudera repo-->
<repositories>
     <repository>
         <id>cloudera</id>
         <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>
     </repository>
 </repositories>

<dependencies>
    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-client</artifactId>
        <version>${hbase.version}</version>
    </dependency>
    
    <dependency>
        <groupId>org.apache.hbase</groupId>
        <artifactId>hbase-server</artifactId>
        <version>${hbase.version}</version>
    </dependency>
    
    <!-- hadoop -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>${hadoop.version}</version>
    </dependency>
</dependencies>
```
HBaseUtils.java
```
package com.lihaogn.spark.project.utils;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.client.HBaseAdmin;
import org.apache.hadoop.hbase.client.HTable;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;

/**
 * HBase操作工具类，Java工具类建议采用单例模式封装
 */
public class HBaseUtils {

    HBaseAdmin admin = null;
    Configuration configuration = null;

    /**
     * 私有构造方法
     */
    private HBaseUtils() {

        configuration = new Configuration();
        configuration.set("hbase.zookeeper.quorum", "localhost:2181");
        configuration.set("hbase.rootdir", "hdfs://localhost:8020/hbase");

        try {
            admin = new HBaseAdmin(configuration);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static HBaseUtils instance = null;

    public static synchronized HBaseUtils getInstance() {
        if (null == instance) {
            instance = new HBaseUtils();
        }
        return instance;
    }

    /**
     * 根据表名获取到HTable实例
     *
     * @param tableName
     * @return
     */
    public HTable getTable(String tableName) {
        HTable table = null;
        try {
            table = new HTable(configuration, tableName);
        } catch (IOException e) {
            e.printStackTrace();
        }
        return table;
    }

    /**
     * 添加一条记录到表中
     *
     * @param tableName
     * @param rowkey
     * @param cf
     * @param column
     * @param value
     */
    public void put(String tableName, String rowkey, String cf, String column, String value) {
        HTable table = getTable(tableName);

        Put put = new Put(Bytes.toBytes(rowkey));
        put.add(Bytes.toBytes(cf), Bytes.toBytes(column), Bytes.toBytes(value));

        try {
            table.put(put);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public static void main(String[] args) {
//        HTable table = HBaseUtils.getInstance().getTable("course_clickcount");
//        System.out.println(table.getName().getNameAsString());

        String tableName = "course_clickcount";
        String rowkey = "20180906_1";
        String cf = "info";
        String column = "click_count";
        String value = "2";

        HBaseUtils.getInstance().put(tableName, rowkey, cf, column, value);
    }
}
```

3）数据库操作

CourseClickCountDAO.scala
```
package com.lihaogn.sparkProject.dao

import com.lihaogn.spark.project.utils.HBaseUtils
import com.lihaogn.sparkProject.domain.CourseClickCount
import org.apache.hadoop.hbase.client.Get
import org.apache.hadoop.hbase.util.Bytes

import scala.collection.mutable.ListBuffer

/**
  * 数据访问层，课程点击数
  */
object CourseClickCountDAO {

  val tableName = "course_clickcount"
  val cf = "info"
  val qualifer = "click_count"

  /**
    * 保存数据到HBase
    *
    * @param list
    */
  def save(list: ListBuffer[CourseClickCount]): Unit = {

    val table = HBaseUtils.getInstance().getTable(tableName)

    for (ele <- list) {
      table.incrementColumnValue(Bytes.toBytes(ele.day_course),
        Bytes.toBytes(cf),
        Bytes.toBytes(qualifer),
        ele.click_count)
    }
  }

  /**
    * 根据rowkey查询值
    * @param day_course
    * @return
    */
  def count(day_course:String):Long= {
    val table = HBaseUtils.getInstance().getTable(tableName)

    val get = new Get(Bytes.toBytes(day_course))
    val value = table.get(get).getValue(cf.getBytes, qualifer.getBytes)

    if (value == null) {
      0L
    } else
      Bytes.toLong(value)
  }

  def main(args: Array[String]): Unit = {
    val list=new ListBuffer[CourseClickCount]
    list.append(CourseClickCount("20180906_8",8))
    list.append(CourseClickCount("20180906_4",3))
    list.append(CourseClickCount("20180906_2",2))

    save(list)

    println(count("20180906_8")+":"+count("20180906_4")+":"+count("20180906_2"))
  }

}
```

CourseSearchClickCountDAO.scala
```
package com.lihaogn.sparkProject.dao

import com.lihaogn.spark.project.utils.HBaseUtils
import com.lihaogn.sparkProject.domain.{CourseClickCount, CourseSearchClickCount}
import org.apache.hadoop.hbase.client.Get
import org.apache.hadoop.hbase.util.Bytes

import scala.collection.mutable.ListBuffer

/**
  * 数据访问层，从搜索引擎过来的课程点击数
  */
object CourseSearchClickCountDAO {

  val tableName = "course_search_clickcount"
  val cf = "info"
  val qualifer = "click_count"

  /**
    * 保存数据到HBase
    *
    * @param list
    */
  def save(list: ListBuffer[CourseSearchClickCount]): Unit = {

    val table = HBaseUtils.getInstance().getTable(tableName)

    for (ele <- list) {
      table.incrementColumnValue(Bytes.toBytes(ele.day_search_course),
        Bytes.toBytes(cf),
        Bytes.toBytes(qualifer),
        ele.click_count)
    }
  }

  /**
    * 根据rowkey查询值
    *
    * @param day_search_course
    * @return
    */
  def count(day_search_course: String): Long = {
    val table = HBaseUtils.getInstance().getTable(tableName)

    val get = new Get(Bytes.toBytes(day_search_course))
    val value = table.get(get).getValue(cf.getBytes, qualifer.getBytes)

    if (value == null) {
      0L
    } else
      Bytes.toLong(value)
  }

  def main(args: Array[String]): Unit = {
    val list = new ListBuffer[CourseSearchClickCount]
    list.append(CourseSearchClickCount("20180906_www.baidu.com_8", 8))
    list.append(CourseSearchClickCount("20180906_www.baidu.com_4", 3))

    save(list)

    println(count("20180906_www.baidu.com_8") + ":" + count("20180906_www.baidu.com_4"))
  }

}
```

4）主类

SparkStreamingApp.scala
```
package com.lihaogn.sparkProject.main

import com.lihaogn.sparkProject.dao.{CourseClickCountDAO, CourseSearchClickCountDAO}
import com.lihaogn.sparkProject.domain.{ClickLog, CourseClickCount, CourseSearchClickCount}
import com.lihaogn.sparkProject.utils.DateUtils
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.kafka.KafkaUtils

import scala.collection.mutable.ListBuffer

/**
  * 使用spark streaming分析日志
  */
object SparkStreamingApp {

  def main(args: Array[String]): Unit = {

    if (args.length != 4) {
      System.err.println("usage: KafKaReceiverWC <zkQuorum> <group> <topics> <numThreads>")
    }

    val Array(zkQuorum, group, topics, numThreads) = args

    val sparkConf = new SparkConf().setAppName("SparkStreamingApp").setMaster("local[5]")

    val ssc = new StreamingContext(sparkConf, Seconds(5))

    val topicMap = topics.split(",").map((_, numThreads.toInt)).toMap

    // spark streaming 对接 kafka
    val messages = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)

    // 步骤一：测试数据接收
    messages.map(_._2).count().print()

    // 步骤二：数据清洗
    val logs = messages.map(_._2)
    val cleanData = logs.map(line => {
      val infos = line.split("\t")

      val url = infos(2).split(" ")(1)
      var courseId = 0

      // 获取课程标号
      if (url.startsWith("/class")) {
        val courseHtml = url.split("/")(2)
        courseId = courseHtml.substring(0, courseHtml.lastIndexOf(".")).toInt
      }

      ClickLog(infos(0), DateUtils.parseToMinute(infos(1)), courseId, infos(3).toInt, infos(4))
    }).filter(clicklog => clicklog.courseId != 0)

    cleanData.print()

    // 步骤三：统计今天到现在为止的课程访问量
    cleanData.map(x=>{
      (x.time.substring(0,8)+"_"+x.courseId,1)
    }).reduceByKey(_+_).foreachRDD(rdd=>{
      rdd.foreachPartition(partitionRecords=>{
        val list=new ListBuffer[CourseClickCount]

        partitionRecords.foreach(pair=>{
          list.append(CourseClickCount(pair._1,pair._2))
        })
        // 写入数据库
        CourseClickCountDAO.save(list)

      })
    })

    // 步骤四：统计从搜索引擎过来的从今天开始到现在的课程的访问量
    cleanData.map(x=>{
      val referer=x.referer.replaceAll("//","/")
      val splits=referer.split("/")
      var host=""
      if(splits.length>2) {
        host=splits(1)
      }

      (host,x.courseId,x.time)
    }).filter(_._1!="").map(x=>{
      (x._3.substring(0,8)+"_"+x._1+"_"+x._2,1)
    }).reduceByKey(_+_).foreachRDD(rdd=>{
      rdd.foreachPartition(partitionRecords=>{
        val list =new ListBuffer[CourseSearchClickCount]

        partitionRecords.foreach(pair=>{
          list.append(CourseSearchClickCount(pair._1,pair._2))
        })
        // 写入数据库
        CourseSearchClickCountDAO.save(list)

      })
    })

    ssc.start()

    ssc.awaitTermination()
  }
}
```
### 5 运行测试

1）启动 zookeeper
```
zkServer.sh start
```
2）启动 HDFS
```
start-dfs.sh
start-yarn.sh
```
3）启动 kafka

>> kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties

监听kafka

>> kafka-console-consumer.sh --zookeeper hadoop000:2181 --topic test_topic

4）启动 flume
```
flume-ng agent \
--conf $FLUME_HOME/conf \
--conf-file $FLUME_HOME/conf/streaming_project2.conf \
--name exec-memory-kafka \
-Dflume.root.logger=INFO,console
```
5）运行日志生成器
>> python3 /home/hadoop/data/generate_log.py

>> python /home/hadoop/data/generate_log.py

6）运行spark程序
```
spark-submit \
--class com.lihaogn.sparkProject.main.SparkStreamingApp \
--master local[5] \
--name SparkStreamingApp \
--jars /home/hadoop/app/lib/spark-streaming-kafka-0-8-assembly_2.11-2.2.0.jar,$(echo /home/hadoop/app/hbase-1.2.0-cdh5.7.0/lib/*.jar | tr ' ' ',') \
/home/hadoop/app/my-lib/SparkStreamingTrain-1.0.jar \
hadoop000:2181 test test_topic 1
```

7）结果 
![image](https://github.com/leelovejava/doc/blob/master/img/project/02.png?raw=true)

## （二）数据可视化
### 1 需求
使用echarts可视化工具将之前统计好的数据进行展示。

### 2 开发环境
IDEA+maven
spring boot + ECharts

### 3 最终效果

![image](https://github.com/leelovejava/doc/blob/master/img/project/01.png)

### 4 编程
1）pom.xml，添加依赖
```xml
<repositories>
    <repository>
        <id>cloudera</id>
        <url>https://repository.cloudera.com/artifactory/cloudera-repos</url>
    </repository>
</repositories>

<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-thymeleaf</artifactId>
</dependency>

<dependency>
    <groupId>org.codehaus.groovy</groupId>
    <artifactId>groovy</artifactId>
    <version>2.5.0</version>
</dependency>

<!-- hbase -->
<dependency>
    <groupId>org.apache.hbase</groupId>
    <artifactId>hbase-client</artifactId>
    <version>${hbase.version}</version>
</dependency>

<!-- json -->
<dependency>
    <groupId>net.sf.json-lib</groupId>
    <artifactId>json-lib</artifactId>
    <version>2.4</version>
    <classifier>jdk15</classifier>
</dependency>
```
2）echarts.html，展示页面
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <title>imooc_stat</title>

    <!-- 引入 ECharts 文件 -->
    <script src="js/echarts.min.js"></script>

    <!-- 引入 jQuery 文件 -->
    <script src="js/jquery.js"></script>
</head>
<body>
<!-- 为 ECharts 准备一个具备大小（宽高）的 DOM -->
<div id="main" style="width: 600px;height:400px;position: absolute; top:50%; left: 50%; margin-top: -200px;margin-left: -300px"></div>


<script type="text/javascript">
    // 基于准备好的dom，初始化echarts实例
    let myChart = echarts.init(document.getElementById('main'));

    // 指定图表的配置项和数据
    let option = {
        title : {
            text: '慕课网实战课程实时访问量统计',
            subtext: '实战课程访问次数',
            x:'center'
        },
        tooltip : {
            trigger: 'item',
            formatter: "{a} <br/>{b} : {c} ({d}%)"
        },
        // legend: {
        //     orient: 'vertical',
        //     left: 'left'
        // },
        series : [
            {
                name: '访问次数',
                type: 'pie',
                radius : '55%',
                center: ['50%', '60%'],
                data: (function(){ //<![CDATA[
                    var datas = [];
                    $.ajax({
                        type: "POST",
                        url: "/lihaogn/course_clickcount",
                        dataType: 'json',
                        async: false,
                        success: function(result) {
                            for(let i=0; i<result.length; i++) {
                                datas.push({"value":result[i].value, "name":result[i].name})
                            }
                        }
                    });
                    return datas;
                    //]]>
                })(),
                itemStyle: {
                    emphasis: {
                        shadowBlur: 10,
                        shadowOffsetX: 0,
                        shadowColor: 'rgba(0, 0, 0, 0.5)'
                    }
                }
            }
        ]
    };
    // 使用刚指定的配置项和数据显示图表。
    myChart.setOption(option);
</script>
</body>
</html>
```
3）HBaseUtils.java，操作数据库
```
package com.lihaogn.spark.web.com.lihaogn.spark.web.utils;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.hbase.client.*;
import org.apache.hadoop.hbase.filter.Filter;
import org.apache.hadoop.hbase.filter.PrefixFilter;
import org.apache.hadoop.hbase.util.Bytes;

import java.io.IOException;
import java.util.HashMap;
import java.util.Map;

/**
 * HBase操作工具类
 */
public class HBaseUtils {


    HBaseAdmin admin = null;
    Configuration conf = null;


    /**
     * 私有构造方法：加载一些必要的参数
     */
    private HBaseUtils() {
        conf = new Configuration();
        conf.set("hbase.zookeeper.quorum", "localhost:2181");
        conf.set("hbase.rootdir", "hdfs://localhost:8020/hbase");

        try {
            admin = new HBaseAdmin(conf);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private static HBaseUtils instance = null;

    public static synchronized HBaseUtils getInstance() {
        if (null == instance) {
            instance = new HBaseUtils();
        }
        return instance;
    }

    /**
     * 根据表名获取到HTable实例
     */
    public HTable getTable(String tableName) {
        HTable table = null;

        try {
            table = new HTable(conf, tableName);
        } catch (IOException e) {
            e.printStackTrace();
        }
        return table;
    }


    /**
     * 根据表名和输入条件获取HBase的记录数
     */
    public Map<String, Long> query(String tableName, String condition) throws Exception {

        Map<String, Long> map = new HashMap<String, Long>();

        HTable table = getTable(tableName);
        String cf = "info";
        String qualifier = "click_count";

        Scan scan = new Scan();

        Filter filter = new PrefixFilter(Bytes.toBytes(condition));
        scan.setFilter(filter);

        ResultScanner rs = table.getScanner(scan);

        for (Result result : rs) {
            String row = Bytes.toString(result.getRow());
            long clickCount = Bytes.toLong(result.getValue(cf.getBytes(),qualifier.getBytes()));
            map.put(row, clickCount);

        }

        return map;
    }


    public static void main(String[] args) throws Exception {

//        HTable table = HBaseUtils.getInstance().getTable("course_clickcount");
//        System.out.println(table.getName().getNameAsString());

        Map<String, Long> map = HBaseUtils.getInstance().query("course_clickcount", "20180906_");

        for (Map.Entry<String, Long> entry : map.entrySet()) {
            System.out.println(entry.getKey() + ":" + entry.getValue());
        }
    }

}
```
4）CourseClickCount.java，实体类
```
package com.lihaogn.spark.web.com.lihaogn.spark.web.domain;

import org.springframework.stereotype.Component;

/**
 * 课程点击数实体类
 */
@Component
public class CourseClickCount {
    private String name;
    private long value;

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public long getValue() {
        return value;
    }

    public void setValue(long value) {
        this.value = value;
    }
}
```

5）CourseClickCountDAO.java，访问数据库
```java
package com.lihaogn.spark.web.com.lihaogn.spark.web.dao;

import com.lihaogn.spark.web.com.lihaogn.spark.web.domain.CourseClickCount;
import com.lihaogn.spark.web.com.lihaogn.spark.web.utils.HBaseUtils;
import org.springframework.stereotype.Component;

import java.util.ArrayList;
import java.util.List;
import java.util.Map;

/**
 * 课程访问数量数据访问
 */
@Component
public class CourseClickCountDAO {

    /**
     * 根据日期查询
     *
     * @param day
     * @return
     * @throws Exception
     */
    public List<CourseClickCount> query(String day) throws Exception {
        List<CourseClickCount> list = new ArrayList<>();

        // 去HBase表中根据day获取课程访问量
        Map<String, Long> map = HBaseUtils.getInstance().query("course_clickcount", "20180906");

        for (Map.Entry<String, Long> entry : map.entrySet()) {
            CourseClickCount model = new CourseClickCount();
            model.setName(entry.getKey());
            model.setValue(entry.getValue());

            list.add(model);

        }
        return list;
    }

    public static void main(String[] args) throws Exception {

        CourseClickCountDAO dao = new CourseClickCountDAO();
        List<CourseClickCount> list = dao.query("20180906");
        for (CourseClickCount model : list) {
            System.out.println(model.getName() + " : " + model.getValue());
        }

    }

}
```

6）SparkStatApp.java，web后端
```java
package com.lihaogn.spark.web.com.lihaogn.spark.web.spark;

import com.lihaogn.spark.web.com.lihaogn.spark.web.dao.CourseClickCountDAO;
import com.lihaogn.spark.web.com.lihaogn.spark.web.domain.CourseClickCount;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestMethod;
import org.springframework.web.bind.annotation.ResponseBody;
import org.springframework.web.bind.annotation.RestController;
import org.springframework.web.servlet.ModelAndView;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * web
 */
@RestController
public class SparkStatApp {

    private static Map<String, String> courses = new HashMap<>();

    static {
        courses.put("112", "Spark SQL慕课网日志分析");
        courses.put("128", "10小时入门大数据");
        courses.put("145", "深度学习之神经网络核心原理与算法");
        courses.put("146", "强大的Node.js在Web开发的应用");
        courses.put("131", "Vue+Django实战");
        courses.put("130", "Web前端性能优化");
    }

    @Autowired
    CourseClickCountDAO courseClickCountDAO;

    @RequestMapping(value = "/course_clickcount",method = RequestMethod.POST)
    @ResponseBody
    public List<CourseClickCount> courseClickCount() throws Exception {

        List<CourseClickCount> list = courseClickCountDAO.query("20180906");
        for (CourseClickCount model : list) {
            model.setName(courses.get(model.getName().substring(9)));
        }
        return list;
    }


    @RequestMapping(value = "/echarts", method = RequestMethod.GET)
    public ModelAndView echarts() {
        return new ModelAndView("echarts");
    }
}
```

7）WebApplication.java，主程序
```java
package com.lihaogn.spark.web;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class WebApplication {

    public static void main(String[] args) {
        SpringApplication.run(WebApplication.class, args);
    }
}
```
--------------------------------------------------------------

数据可视化
https://github.com/lihaogm/spark-echarts-log.git